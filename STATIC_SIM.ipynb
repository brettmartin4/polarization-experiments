{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cc160-4ad3-4c7c-9b1d-d22a79590190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STATIC SIMULATION FOR THRESHOLD EXCEEDANCE ---\n",
    "# Works with sklearn .pkl or Keras .h5 models\n",
    "# Assumes you already have a dataframe `df` with weather/time/polarization features\n",
    "# and a raw fidelity column for ground-truth lookahead.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# If you plan to load .pkl models:\n",
    "import joblib\n",
    "\n",
    "# If you plan to load .h5 models:\n",
    "try:\n",
    "    from tensorflow.keras.models import load_model as keras_load_model\n",
    "except Exception:\n",
    "    keras_load_model = None  # only needed if you actually load a .h5\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edit these)\n",
    "# =========================\n",
    "MODEL_PATH = \"model.pkl\"     # e.g., \"xgb.pkl\", \"svr.pkl\", or \"nn.h5\"\n",
    "FEATS      = [               # exact feature list used in training, in order\n",
    "    # e.g., \"temp_x\", \"temp_y\", \"tod_sin\", \"tod_cos\", \"wind_u_x\", ...\n",
    "    # Fill with your training-time feature names:\n",
    "]\n",
    "FID_COL    = \"Fidelity\"      # raw polarization fidelity column for ground-truth lookahead\n",
    "TARGET_KIND = \"rstdev\"       # one of: \"rstdev\", \"rextdiff\", \"absdiff\"\n",
    "THRESH     = 0.10            # absolute fidelity-drift tolerance (example)\n",
    "HORIZON_H  = 300             # how far ahead to look (in samples) for a true exceedance\n",
    "RSTDEV_WINDOW = 300          # model’s intended window length for std (if relevant)\n",
    "GAUSS_P_EXCEED_THRESH = 0.05 # for rstdev: threshold on predicted exceedance percent -> flag\n",
    "\n",
    "# Optional: standardize X for Keras models if they were trained on standardized inputs\n",
    "APPLY_SCALER = False\n",
    "SCALER_MEAN = None  # set np.array of means if you saved them\n",
    "SCALER_STD  = None  # set np.array of stds   if you saved them\n",
    "\n",
    "# Optional: post-processing for model outputs (if you trained on a scaled y)\n",
    "Y_MEAN = None\n",
    "Y_STD  = None\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "def load_any_model(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".pkl\":\n",
    "        return joblib.load(path), \"sklearn\"\n",
    "    elif ext == \".h5\":\n",
    "        if keras_load_model is None:\n",
    "            raise RuntimeError(\"TensorFlow/Keras not available to load .h5 models.\")\n",
    "        return keras_load_model(path), \"keras\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model extension: {ext}\")\n",
    "\n",
    "def predict_any(model, kind, X):\n",
    "    if kind == \"sklearn\":\n",
    "        yhat = model.predict(X)\n",
    "    else:\n",
    "        # Keras expects 2D (N, F). If your NN expects sequences, adapt here.\n",
    "        yhat = model.predict(X, verbose=0).ravel()\n",
    "    return np.asarray(yhat, dtype=float)\n",
    "\n",
    "def maybe_unscale_y(yhat):\n",
    "    if (Y_MEAN is not None) and (Y_STD is not None) and (Y_STD != 0):\n",
    "        return yhat * float(Y_STD) + float(Y_MEAN)\n",
    "    return yhat\n",
    "\n",
    "def gaussian_exceed_prob(sigma, tol):\n",
    "    \"\"\"Two-sided exceedance P(|X|>tol) for N(0, sigma^2).\"\"\"\n",
    "    if sigma <= 0:\n",
    "        return 0.0\n",
    "    z = tol / sigma\n",
    "    # 1 - Phi(z) using error function\n",
    "    tail = 0.5 * math.erfc(z / math.sqrt(2.0))\n",
    "    return 2.0 * tail\n",
    "\n",
    "def chebyshev_upper_bound(sigma, tol):\n",
    "    \"\"\"Chebyshev upper bound on P(|X - mu| >= tol) <= (sigma/tol)^2.\"\"\"\n",
    "    if sigma <= 0:\n",
    "        return 0.0\n",
    "    r = sigma / tol\n",
    "    return float(min(1.0, r * r))\n",
    "\n",
    "def ground_truth_exceed_in_horizon(fid, idx, tol, H):\n",
    "    \"\"\"\n",
    "    True label for a given index: does |fidelity[t+h] - fidelity[t]| exceed tol at\n",
    "    any step 1..H? (You can change the reference if needed.)\n",
    "    \"\"\"\n",
    "    ref = fid[idx]\n",
    "    end = min(len(fid), idx + H + 1)\n",
    "    if end <= idx + 1:\n",
    "        return 0\n",
    "    segment = fid[idx+1:end]\n",
    "    return int(np.any(np.abs(segment - ref) > tol))\n",
    "\n",
    "def split_train_val_test(n, tr=0.70, va=0.15):\n",
    "    n_tr = int(n * tr)\n",
    "    n_va = int(n * va)\n",
    "    n_te = n - n_tr - n_va\n",
    "    return slice(0, n_tr), slice(n_tr, n_tr + n_va), slice(n_tr + n_va, n)\n",
    "\n",
    "# =========================\n",
    "# MAIN SIMULATION\n",
    "# =========================\n",
    "# 0) Basic checks\n",
    "assert isinstance(df, pd.DataFrame), \"Expected a DataFrame named `df` to be present.\"\n",
    "for c in FEATS + [FID_COL]:\n",
    "    if c not in df.columns:\n",
    "        raise KeyError(f\"Column `{c}` not in df.\")\n",
    "\n",
    "# 1) Prep features/labels (test split only)\n",
    "X_all = df[FEATS].copy()\n",
    "fid   = df[FID_COL].astype(float).to_numpy()\n",
    "\n",
    "idx_tr, idx_va, idx_te = split_train_val_test(len(df))\n",
    "X_te = X_all.iloc[idx_te].reset_index(drop=True)\n",
    "fid_te = fid[idx_te]\n",
    "\n",
    "if APPLY_SCALER:\n",
    "    if SCALER_MEAN is None or SCALER_STD is None:\n",
    "        raise RuntimeError(\"APPLY_SCALER=True but SCALER_MEAN/SCALER_STD not provided.\")\n",
    "    X_te = (X_te.to_numpy(dtype=float) - SCALER_MEAN) / SCALER_STD\n",
    "else:\n",
    "    X_te = X_te.to_numpy(dtype=float)\n",
    "\n",
    "# 2) Load model and predict on test features\n",
    "model, model_kind = load_any_model(MODEL_PATH)\n",
    "yhat_te = predict_any(model, model_kind, X_te)\n",
    "yhat_te = maybe_unscale_y(yhat_te)\n",
    "\n",
    "# 3) Interpretation layer → predicted exceed flag (and extras for rstdev)\n",
    "pred_flag = np.zeros(len(yhat_te), dtype=int)\n",
    "gauss_p   = np.full(len(yhat_te), np.nan)\n",
    "cheb_p    = np.full(len(yhat_te), np.nan)\n",
    "\n",
    "if TARGET_KIND == \"rstdev\":\n",
    "    # yhat = predicted sigma over a window; convert to exceedance probabilities\n",
    "    for i, s in enumerate(yhat_te):\n",
    "        gp = gaussian_exceed_prob(s, THRESH)\n",
    "        cb = chebyshev_upper_bound(s, THRESH)\n",
    "        gauss_p[i] = gp\n",
    "        cheb_p[i]  = cb\n",
    "        pred_flag[i] = int(gp >= GAUSS_P_EXCEED_THRESH)\n",
    "\n",
    "elif TARGET_KIND == \"rextdiff\":\n",
    "    # yhat = predicted peak-to-peak over a window\n",
    "    pred_flag = (yhat_te > THRESH).astype(int)\n",
    "\n",
    "elif TARGET_KIND == \"absdiff\":\n",
    "    # yhat = predicted absolute first-difference at the next step\n",
    "    pred_flag = (yhat_te > THRESH).astype(int)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"TARGET_KIND must be one of {'rstdev','rextdiff','absdiff'}.\")\n",
    "\n",
    "# 4) Ground-truth labels via lookahead in the *actual fidelity*\n",
    "true_flag = np.zeros(len(fid_te), dtype=int)\n",
    "for i in range(len(fid_te)):\n",
    "    true_flag[i] = ground_truth_exceed_in_horizon(fid_te, i, THRESH, HORIZON_H)\n",
    "\n",
    "# 5) Align lengths (if model produced fewer predictions for any reason)\n",
    "m = min(len(pred_flag), len(true_flag))\n",
    "pred_flag = pred_flag[:m]\n",
    "true_flag = true_flag[:m]\n",
    "if TARGET_KIND == \"rstdev\":\n",
    "    gauss_p = gauss_p[:m]\n",
    "    cheb_p  = cheb_p[:m]\n",
    "\n",
    "# 6) Metrics\n",
    "cm = confusion_matrix(true_flag, pred_flag, labels=[0,1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"=== Confusion Matrix (rows=true, cols=pred) ===\")\n",
    "print(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(true_flag, pred_flag, digits=3))\n",
    "\n",
    "if TARGET_KIND == \"rstdev\":\n",
    "    print(f\"\\nAvg predicted Gaussian exceedance P(|X|>{THRESH}): {np.nanmean(gauss_p):.4f}\")\n",
    "    print(f\"Avg predicted Chebyshev upper bound:            {np.nanmean(cheb_p):.4f}\")\n",
    "\n",
    "# 7) Optional: regression diagnostics of raw predictions vs simple targets\n",
    "#    (only meaningful if your model target is comparable to a numeric truth)\n",
    "try:\n",
    "    rmse = lambda a,b: float(np.sqrt(mean_squared_error(a,b)))\n",
    "    print(\"\\n=== Regression Diagnostics (if applicable) ===\")\n",
    "    print(f\"Sample size used: {m}\")\n",
    "    print(\"Note: These RMSE/R^2 values are meaningful only if yhat is directly comparable to a numeric truth.\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class StaticSim:\n",
    "    \"\"\"\n",
    "    Streaming/static rollout simulator.\n",
    "    - step(...) consumes ONE observation (row) at a time.\n",
    "    - Maintains internal state (buffers, accumulators).\n",
    "    - Main program handles visualization/printing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        # Required config\n",
    "        self.pred_method = args[\"pred_method\"]      # \"rstdev\", \"rexd\", \"fder\", \"fder_sder\"\n",
    "        self.target      = args.get(\"pred_targ\", \"\")# informational\n",
    "        self.thresh      = float(args[\"thresh\"])    # e.g., 0.1\n",
    "        self.win_size    = int(args[\"tol\"])         # window length in steps (for buffers)\n",
    "        self.sr          = float(args[\"sr\"])        # sample rate in seconds per step (dt)\n",
    "        self.dt          = self.sr                  # alias\n",
    "\n",
    "        # For rstdev: probability threshold\n",
    "        # If you want \"flag when P(|dev|>thresh) >= p0\", set p0 here.\n",
    "        # Your old gauss_thresh = thresh/2 is dimensionally weird; better is prob threshold.\n",
    "        self.p_exceed_thresh = float(args.get(\"p_exceed_thresh\", 0.2))  # e.g., 20%\n",
    "\n",
    "        # For derivative horizon checks\n",
    "        self.horizon_steps = int(args.get(\"horizon_steps\", 5))          # e.g., 5 steps ahead\n",
    "        self.H = self.horizon_steps * self.dt\n",
    "\n",
    "        # For total-variation accumulation (streaming)\n",
    "        self.use_tv = bool(args.get(\"use_total_variation\", True))\n",
    "        self.use_horizon = bool(args.get(\"use_horizon_forecast\", True))\n",
    "\n",
    "        # If using EMA for variation instead of strict window sum:\n",
    "        self.tv_tau_sec = float(args.get(\"tv_tau_sec\", self.win_size * self.dt))\n",
    "        self.tv_alpha = min(1.0, self.dt / self.tv_tau_sec) if self.tv_tau_sec > 0 else 1.0\n",
    "\n",
    "        # Scaling toggles\n",
    "        self.output_scale = bool(args.get(\"output_scaled\", False))\n",
    "        self.input_scale  = bool(args.get(\"input_scaled\",  False))\n",
    "\n",
    "        # Column names\n",
    "        self.posix_feat = args.get(\"posix_feat_name\", \"current_time\")\n",
    "        self.fid_feat   = args.get(\"fid_feat_name\", \"Fidelity\")\n",
    "\n",
    "        # Feature columns used by the model (important for streaming)\n",
    "        self.feature_cols = args.get(\"feature_cols\", None)  # list[str]\n",
    "        if self.feature_cols is None:\n",
    "            raise ValueError(\"args['feature_cols'] must be provided for streaming inference.\")\n",
    "\n",
    "        # Load model(s)\n",
    "        self.load_model(args[\"model_path\"])\n",
    "        self.model_d2 = None\n",
    "        if self.pred_method in (\"fder_sder\", \"d1_d2\", \"fder+sder\"):\n",
    "            self.load_model_d2(args[\"model_path_d2\"])\n",
    "\n",
    "        # Data optional (you can still use run(df))\n",
    "        self.df = None\n",
    "        if \"data_path\" in args:\n",
    "            self.load_data(args[\"data_path\"], self.posix_feat, args.get(\"test_start_posix\", -np.inf))\n",
    "\n",
    "        # Scaling params (optional)\n",
    "        if self.output_scale:\n",
    "            self.output_scale_mean = float(args[\"output_scale_mean\"])\n",
    "            self.output_scale_stdev = float(args[\"output_scale_stdev\"])\n",
    "        if self.input_scale:\n",
    "            self.input_scale_mean = np.array(args[\"input_scale_mean\"], dtype=float)\n",
    "            self.input_scale_stdev = np.array(args[\"input_scale_stdev\"], dtype=float)\n",
    "\n",
    "        # Internal state\n",
    "        self.reset()\n",
    "\n",
    "    # ---------- I/O ----------\n",
    "    def load_model(self, model_path):\n",
    "        # TODO: detect sklearn vs keras; for now keep pickle\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "\n",
    "    def load_model_d2(self, model_path_d2):\n",
    "        with open(model_path_d2, \"rb\") as f:\n",
    "            self.model_d2 = pickle.load(f)\n",
    "\n",
    "    def load_data(self, data_path, posix_name, posix_val):\n",
    "        df = pd.read_csv(data_path)\n",
    "        self.df = df[df[posix_name] >= posix_val].reset_index(drop=True)\n",
    "\n",
    "    # ---------- Scaling ----------\n",
    "    def _scale_inputs(self, x: np.ndarray) -> np.ndarray:\n",
    "        if not self.input_scale:\n",
    "            return x\n",
    "        return (x - self.input_scale_mean) / self.input_scale_stdev\n",
    "\n",
    "    def _unscale_output(self, yhat: float) -> float:\n",
    "        if not self.output_scale:\n",
    "            return float(yhat)\n",
    "        return float(yhat) * self.output_scale_stdev + self.output_scale_mean\n",
    "\n",
    "    # ---------- State ----------\n",
    "    def reset(self):\n",
    "        # buffers for diagnostics / accumulation\n",
    "        self.t_buf = deque(maxlen=self.win_size)\n",
    "        self.f_buf = deque(maxlen=self.win_size)\n",
    "\n",
    "        self.yhat_buf = deque(maxlen=self.win_size)   # primary prediction history\n",
    "        self.d2_buf   = deque(maxlen=self.win_size)   # if used\n",
    "\n",
    "        # For total-variation tracking from |d1|\n",
    "        self.tv_ema = 0.0\n",
    "        self.tv_window = deque(maxlen=self.win_size)  # store per-step contributions |d1|*dt\n",
    "\n",
    "        # counters\n",
    "        self.k = 0\n",
    "\n",
    "    # ---------- Core step ----------\n",
    "    def step(self, obs) -> dict:\n",
    "        \"\"\"\n",
    "        obs: dict-like or pd.Series with at least:\n",
    "            - posix time column (self.posix_feat)\n",
    "            - Fidelity column (self.fid_feat) [optional for model, useful for logging]\n",
    "            - all model feature columns (self.feature_cols)\n",
    "        Returns a dict with predictions and flags for this step.\n",
    "        \"\"\"\n",
    "        if isinstance(obs, pd.Series):\n",
    "            obs = obs.to_dict()\n",
    "\n",
    "        t = obs.get(self.posix_feat, None)\n",
    "        f = obs.get(self.fid_feat, None)\n",
    "\n",
    "        # Build feature vector for model\n",
    "        x = np.array([obs[c] for c in self.feature_cols], dtype=float)\n",
    "        x = self._scale_inputs(x).reshape(1, -1)\n",
    "\n",
    "        # Predict primary target\n",
    "        yhat = float(self.model.predict(x).reshape(-1)[0])\n",
    "        yhat = self._unscale_output(yhat)\n",
    "\n",
    "        # Optional second-derivative prediction (absolute)\n",
    "        yhat_d2 = None\n",
    "        if self.model_d2 is not None:\n",
    "            yhat_d2 = float(self.model_d2.predict(x).reshape(-1)[0])\n",
    "            # you may need its own unscale params if trained differently\n",
    "\n",
    "        # Update internal buffers\n",
    "        self.t_buf.append(t)\n",
    "        self.f_buf.append(f)\n",
    "        self.yhat_buf.append(yhat)\n",
    "        if yhat_d2 is not None:\n",
    "            self.d2_buf.append(yhat_d2)\n",
    "\n",
    "        # Compute flag + additional metrics depending on method\n",
    "        out = {\n",
    "            \"step\": self.k,\n",
    "            \"t\": t,\n",
    "            \"Fidelity\": f,\n",
    "            \"yhat\": yhat,\n",
    "        }\n",
    "        if yhat_d2 is not None:\n",
    "            out[\"yhat_d2\"] = yhat_d2\n",
    "\n",
    "        flag_info = self.update_flags_stream(yhat, yhat_d2=yhat_d2)\n",
    "        out.update(flag_info)\n",
    "\n",
    "        self.k += 1\n",
    "        return out\n",
    "\n",
    "    # ---------- Streaming flags ----------\n",
    "    def update_flags_stream(self, yhat, yhat_d2=None) -> dict:\n",
    "        method_name = f\"_flags_{self.pred_method}\"\n",
    "        if not hasattr(self, method_name):\n",
    "            raise ValueError(f\"No update method exists for {method_name}\")\n",
    "        return getattr(self, method_name)(yhat, yhat_d2=yhat_d2)\n",
    "\n",
    "    # --- rstdev: turn σ into exceedance probability ---\n",
    "    def _flags_rstdev(self, s, yhat_d2=None) -> dict:\n",
    "        s = float(s)\n",
    "        if s <= 0:\n",
    "            p_gauss = 0.0\n",
    "            p_cheby = 0.0\n",
    "        else:\n",
    "            z = self.thresh / s\n",
    "            tail = 0.5 * math.erfc(z / math.sqrt(2.0))     # 1 - Phi(z)\n",
    "            p_gauss = 2.0 * tail                           # P(|X|>T) for N(0,s^2)\n",
    "            r = s / self.thresh\n",
    "            p_cheby = float(min(1.0, r * r))               # Chebyshev upper bound\n",
    "\n",
    "        flag = int(p_gauss >= self.p_exceed_thresh)\n",
    "        return {\n",
    "            \"flag\": flag,\n",
    "            \"p_exceed_gauss\": p_gauss,\n",
    "            \"p_exceed_cheby_ub\": p_cheby,\n",
    "        }\n",
    "\n",
    "    # --- rexd: direct excursion ---\n",
    "    def _flags_rexd(self, r, yhat_d2=None) -> dict:\n",
    "        r = float(r)\n",
    "        return {\"flag\": int(r > self.thresh), \"deltaF_est\": r}\n",
    "\n",
    "    # --- fder: |dF/dt| (absolute) ---\n",
    "    def _flags_fder(self, abs_d1, yhat_d2=None) -> dict:\n",
    "        abs_d1 = float(abs_d1)\n",
    "\n",
    "        # 1) Horizon forecast upper bound\n",
    "        delta_h = abs_d1 * self.H if self.use_horizon else np.nan\n",
    "        flag_h = int(delta_h > self.thresh) if self.use_horizon else 0\n",
    "\n",
    "        # 2) Total-variation accumulation (windowed or EMA)\n",
    "        tv_contrib = abs_d1 * self.dt\n",
    "        self.tv_window.append(tv_contrib)\n",
    "        tv_win = float(sum(self.tv_window))  # in Fidelity units\n",
    "\n",
    "        # EMA version (smoother, effectively windowed by tau)\n",
    "        self.tv_ema = (1.0 - self.tv_alpha) * self.tv_ema + self.tv_contrib\n",
    "        tv_used = self.tv_ema if self.use_tv else np.nan\n",
    "        flag_tv = int(tv_used > self.thresh) if self.use_tv else 0\n",
    "\n",
    "        # Combine flags (you can choose OR / AND)\n",
    "        flag = int((flag_h == 1) or (flag_tv == 1))\n",
    "\n",
    "        return {\n",
    "            \"flag\": flag,\n",
    "            \"deltaF_horizon_ub\": delta_h,\n",
    "            \"tv_window_sum\": tv_win,\n",
    "            \"tv_ema\": self.tv_ema,\n",
    "            \"flag_horizon\": flag_h,\n",
    "            \"flag_tv\": flag_tv,\n",
    "        }\n",
    "\n",
    "    # --- fder_sder: |d1| + |d2| used for an upper-bound forecast ---\n",
    "    def _flags_fder_sder(self, abs_d1, yhat_d2=None) -> dict:\n",
    "        if yhat_d2 is None:\n",
    "            raise ValueError(\"fder_sder requires yhat_d2 prediction/model.\")\n",
    "\n",
    "        abs_d1 = float(abs_d1)\n",
    "        abs_d2 = float(yhat_d2)\n",
    "\n",
    "        # Upper bound forecast over horizon H: ΔF <= |d1|H + 0.5|d2|H^2\n",
    "        delta_h = abs_d1 * self.H + 0.5 * abs_d2 * (self.H ** 2)\n",
    "        flag_h = int(delta_h > self.thresh)\n",
    "\n",
    "        # Also keep total variation from |d1| (still useful)\n",
    "        tv_contrib = abs_d1 * self.dt\n",
    "        self.tv_window.append(tv_contrib)\n",
    "        self.tv_ema = (1.0 - self.tv_alpha) * self.tv_ema + self.tv_contrib\n",
    "        flag_tv = int(self.tv_ema > self.thresh) if self.use_tv else 0\n",
    "\n",
    "        flag = int((flag_h == 1) or (flag_tv == 1))\n",
    "\n",
    "        return {\n",
    "            \"flag\": flag,\n",
    "            \"deltaF_horizon_ub\": delta_h,\n",
    "            \"tv_ema\": self.tv_ema,\n",
    "            \"flag_horizon\": flag_h,\n",
    "            \"flag_tv\": flag_tv,\n",
    "        }\n",
    "\n",
    "    # ---------- Convenience: batch run ----------\n",
    "    def run_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.reset()\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            rows.append(self.step(row))\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"No data loaded. Use run_df(df) or provide data_path in args.\")\n",
    "        return self.run_df(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path: ./model.pkl\n",
    "data_path: ./data.csv\n",
    "pred_targ: RSTDEV_Fidelity_300_SAVGOL\n",
    "features:\n",
    " - x\n",
    "pred_method: rstdev # rstdev, rexd, fder\n",
    "test_start_posix: 2\n",
    "posix_feat_name: current_time\n",
    "tol: 300 # aggregation window size (observations)\n",
    "sr: 30 # sample rate (seconds)\n",
    "thresh: 0.05\n",
    "output_scaled: False\n",
    "output_scale_mean: 0\n",
    "output_scale_stdev: 0\n",
    "input_scaled: False\n",
    "input_scale_mean: 0\n",
    "input_scale_stdev: 0\n",
    "plot: False\n",
    "poincare: False\n",
    "\n",
    "p_exceed_thresh: 0.2     # flag if P(|dev| > thresh) >= 20%\n",
    "horizon_steps: 5         # predict threshold crossing within next 5 steps\n",
    "tv_tau_sec: 900          # EMA timescale for total-variation accumulation (optional)\n",
    "use_total_variation: True\n",
    "use_horizon_forecast: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(yaml_path: str) -> dict:\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # Normalize key names used by the simulator\n",
    "    # Your YAML uses `features`, sim uses `feature_cols`\n",
    "    cfg[\"feature_cols\"] = cfg.pop(\"features\", [])\n",
    "\n",
    "    # Optional: if you want derivative horizons / stdev probability thresholds\n",
    "    # cfg.setdefault(\"horizon_steps\", 5)\n",
    "    # cfg.setdefault(\"p_exceed_thresh\", 0.2)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef66ae",
   "metadata": {},
   "source": [
    "Main python file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04200393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .StaticSim import StaticSim\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    sim = StaticSim(args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Please set config yaml path\")\n",
    "        exit(1)\n",
    "    with open(sys.argv[1]) as f:\n",
    "        args = yaml.safe_load(f)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = yaml.safe_load(open(\"./config.yaml\"))\n",
    "sim = main(args)\n",
    "\n",
    "df = pd.read_csv(args[\"data_path\"])\n",
    "df = df[df[args[\"posix_feat_name\"]] >= args[\"test_start_posix\"]].reset_index(drop=True)\n",
    "\n",
    "sim.reset()\n",
    "\n",
    "outs = []\n",
    "for _, row in df.iterrows():\n",
    "    outs.append(sim.step(row))\n",
    "\n",
    "df_sim = pd.DataFrame(outs)\n",
    "df_sim.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
